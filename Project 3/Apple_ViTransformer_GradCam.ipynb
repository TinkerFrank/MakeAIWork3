{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    deprocess_image, \\\n",
    "    preprocess_image\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Insert Saved *.pt Model here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('apple_vit_classifier.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Insert image location of the apple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = 'myapple_2.jpg'\n",
    "img = cv2.imread(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #Switch from BGR to RGB\n",
    "img = cv2.resize(img, (224, 224))\n",
    "img = np.float32(img) / 255\n",
    "input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (features): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): Permute()\n",
       "      (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.018181818181818184, mode=row)\n",
       "        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): PatchMerging(\n",
       "      (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.03636363636363637, mode=row)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05454545454545456, mode=row)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): PatchMerging(\n",
       "      (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07272727272727274, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09090909090909091, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.10909090909090911, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1272727272727273, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.14545454545454548, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.16363636363636364, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): PatchMerging(\n",
       "      (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "      (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.18181818181818182, mode=row)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (permute): Permute()\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the input tensor to the same device as the model\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Ensure that the model's weight tensor is on the same device as the input tensor\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_transform(tensor, height=7, width=7):\n",
    "    result = tensor.reshape(tensor.size(0),\n",
    "                            height, width, tensor.size(2))\n",
    "\n",
    "    # Bring the channels to the first dimension,\n",
    "    # like in CNNs.\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    #result = result.permute(0, 3, 1, 2)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 7, 7, 7]' is invalid for input of size 37632",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m targets \u001b[39m=\u001b[39m [ClassifierOutputTarget(\u001b[39m3\u001b[39m)]\n\u001b[0;32m     12\u001b[0m \u001b[39mwith\u001b[39;00m GradCAM(model\u001b[39m=\u001b[39mmodel, target_layers\u001b[39m=\u001b[39mtarget_layers, reshape_transform\u001b[39m=\u001b[39mreshape_transform) \u001b[39mas\u001b[39;00m cam:\n\u001b[1;32m---> 13\u001b[0m     grayscale_cams \u001b[39m=\u001b[39m cam(input_tensor\u001b[39m=\u001b[39;49minput_tensor, targets\u001b[39m=\u001b[39;49mtargets) \u001b[39m#switched to None as in example: https://github.com/jacobgil/pytorch-grad-cam/blob/master/usage_examples/swinT_example.py\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     cam_image \u001b[39m=\u001b[39m show_cam_on_image(img, grayscale_cams[\u001b[39m0\u001b[39m, :], use_rgb\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m cam \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39muint8(\u001b[39m255\u001b[39m\u001b[39m*\u001b[39mgrayscale_cams[\u001b[39m0\u001b[39m, :])\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\pytorch_grad_cam\\base_cam.py:188\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[1;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[39mif\u001b[39;00m aug_smooth \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_augmentation_smoothing(\n\u001b[0;32m    186\u001b[0m         input_tensor, targets, eigen_smooth)\n\u001b[1;32m--> 188\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(input_tensor,\n\u001b[0;32m    189\u001b[0m                     targets, eigen_smooth)\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\pytorch_grad_cam\\base_cam.py:74\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[1;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_input_gradient:\n\u001b[0;32m     71\u001b[0m     input_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mVariable(input_tensor,\n\u001b[0;32m     72\u001b[0m                                            requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 74\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivations_and_grads(input_tensor)\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     target_categories \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(outputs\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnumpy(), axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\pytorch_grad_cam\\activations_and_gradients.py:42\u001b[0m, in \u001b[0;36mActivationsAndGradients.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradients \u001b[39m=\u001b[39m []\n\u001b[0;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivations \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 42\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torchvision\\models\\swin_transformer.py:608\u001b[0m, in \u001b[0;36mSwinTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 608\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[0;32m    609\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n\u001b[0;32m    610\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute(x)\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torchvision\\models\\swin_transformer.py:453\u001b[0m, in \u001b[0;36mSwinTransformerBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor):\n\u001b[1;32m--> 453\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstochastic_depth(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x)))\n\u001b[0;32m    454\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstochastic_depth(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x)))\n\u001b[0;32m    455\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1557\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1555\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args, kwargs, result)\n\u001b[0;32m   1556\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1557\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, args, result)\n\u001b[0;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m     result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\pytorch_grad_cam\\activations_and_gradients.py:23\u001b[0m, in \u001b[0;36mActivationsAndGradients.save_activation\u001b[1;34m(self, module, input, output)\u001b[0m\n\u001b[0;32m     20\u001b[0m activation \u001b[39m=\u001b[39m output\n\u001b[0;32m     22\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreshape_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     activation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreshape_transform(activation)\n\u001b[0;32m     24\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivations\u001b[39m.\u001b[39mappend(activation\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach())\n",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m, in \u001b[0;36mreshape_transform\u001b[1;34m(tensor, height, width)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape_transform\u001b[39m(tensor, height\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m, width\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     result \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39;49mreshape(tensor\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m),\n\u001b[0;32m      3\u001b[0m                             height, width, tensor\u001b[39m.\u001b[39;49msize(\u001b[39m2\u001b[39;49m))\n\u001b[0;32m      5\u001b[0m     \u001b[39m# Bring the channels to the first dimension,\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[39m# like in CNNs.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 7, 7, 7]' is invalid for input of size 37632"
     ]
    }
   ],
   "source": [
    "# Which target_layer should we chose for Swin Transformers?\n",
    "# Since the swin transformer is different from ViT, it does not contains cls_token as present in ViT, \n",
    "# therefore we will use all the 7x7 images we get from the last block of the last layer.\n",
    "# We should chose any layer before the final attention block\n",
    "# SwinT: model.layers[-1].blocks[-1].norm1\n",
    "target_layers = [model.features[-1][-1].norm1]  #features,  (7): Sequential  (0): SwinTransformerBlock  (norm1)\n",
    "\n",
    "\n",
    "targets = [ClassifierOutputTarget(3)]\n",
    "\n",
    "\n",
    "with GradCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform) as cam:\n",
    "    grayscale_cams = cam(input_tensor=input_tensor, targets=targets) #switched to None as in example: https://github.com/jacobgil/pytorch-grad-cam/blob/master/usage_examples/swinT_example.py\n",
    "    cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "\n",
    "cam = np.uint8(255*grayscale_cams[0, :])\n",
    "cam = cv2.merge([cam, cam, cam])\n",
    "images = np.hstack((np.uint8(255*img), cam , cam_image))\n",
    "Image.fromarray(images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
