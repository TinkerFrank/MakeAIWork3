{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Voeg imports toe als je ze nodig hebt\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch\n",
    "import torchvision.io as tio\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "mylearnrate = 1e-3\n",
    "batchsize = 32\n",
    "mymomentum = 0.9\n",
    "epochs = 50\n",
    "optchoice = 'sgd'\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "inputdata = 'original_augm_cropped' \n",
    "test_batchsize = 32 #total pictures in test\n",
    "limit = 0.01   \n",
    "\n",
    "### TRYING TO GENERALIZE SO THE VAL LOSS DROPS\n",
    "resizevalue = 224 \n",
    "dropout = 0.2 # added extra linear layer to resnet with dropout\n",
    "noise_factor = random.uniform(0, 0.2) # transformer noise \n",
    "myweight_decay=0.001  ##optimizer sgd parameter \n",
    "# L2 regularization, which is the sum of squares of all weights in the model, \n",
    "# and L1 regularization, which is the sum of the absolute values of all weights in the model.\n",
    "# Both of them are scaled by a (small) factor, which is a hyperparameter we set prior to training.\n",
    "# Typically, the parameter for weight decay is set on a logarithmic scale between 0 (overfitting) and 0.1(underftting) \n",
    "# (0.1, 0.01, 0.001, ...)\n",
    "##############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apple_original_augm_cropped/\n"
     ]
    }
   ],
   "source": [
    "if inputdata == 'apple_cropped_r224_augm':\n",
    "    dataset_path = \"./apple_cropped_r224_augm\"\n",
    "elif inputdata == 'resized224':\n",
    "    dataset_path = './apple_resized_224/Train'\n",
    "elif inputdata == 'resized224_augm':\n",
    "    dataset_path = './apple_resized_224_augm/Train'\n",
    "elif inputdata == 'original_augm':\n",
    "    dataset_path = './apple_original_augm/Train'\n",
    "elif inputdata == 'original_augm_cropped':\n",
    "    dataset_path = './apple_original_augm_cropped/'\n",
    "\n",
    "\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blotch_Apple 75\n",
      "Normal_Apple 77\n",
      "Rot_Apple 102\n",
      "Scab_Apple 59\n"
     ]
    }
   ],
   "source": [
    "# get length of each folder in imagefolder dataset\n",
    "\n",
    "import os\n",
    "\n",
    "imagefolder = dataset_path\n",
    "\n",
    "for folder in os.listdir(imagefolder):\n",
    "    print(folder, len(os.listdir(os.path.join(imagefolder, folder))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#If our dataset is more similar to ImageNet dataset, we can use ImageNet mean and std. \n",
    "#ImageNet mean and std are mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# https://towardsdatascience.com/data-augmentations-in-torchvision-5d56d70c372e\n",
    "transform_img_normal = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize([resizevalue ,resizevalue ]),\n",
    "    transforms.Normalize(mean = [0.6453, 0.4631, 0.3085],\n",
    "                          std= [0.2000, 0.2238, 0.2254]),\n",
    "    #transforms.Grayscale(3) #expected is 3 channels for the model, r == g == b\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(dataset_path, transform=transform_img_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a data loader to iterate over the dataset\n",
    "# dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# # Calculate the mean and standard deviation for each channel\n",
    "# channel_means = torch.zeros(3)\n",
    "# channel_stds = torch.zeros(3)\n",
    "# total_samples = 0\n",
    "\n",
    "# for images, _ in dataloader:\n",
    "#     batch_samples = images.size(0)\n",
    "#     channel_means += images.view(batch_samples, 3, -1).mean(2).sum(0) * batch_samples\n",
    "#     channel_stds += images.view(batch_samples, 3, -1).std(2).sum(0) * batch_samples\n",
    "#     total_samples += batch_samples\n",
    "\n",
    "# channel_means /= total_samples\n",
    "# channel_stds /= total_samples\n",
    "\n",
    "# print(\"Channel Means:\", channel_means)\n",
    "# print(\"Channel Stds:\", channel_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Blotch_Apple': 0, 'Normal_Apple': 1, 'Rot_Apple': 2, 'Scab_Apple': 3}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size],generator=generator1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True, num_workers=0)  # numworkers parallel/subprocesses\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=False)  # no need to shuffle when evaluating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    #transforms.RandomCrop(224,224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(180),\n",
    "    #adding noise because the train-loss was going down but the val-loss didnt = overfitting, need to regularize (dropout, more data, noise)\n",
    "    transforms.Lambda(lambda x: x + torch.randn(x.size()) * noise_factor), \n",
    "    #normalizing after adding noise\n",
    "    transforms.Normalize(mean = [0.6453, 0.4631, 0.3085],\n",
    "                          std= [0.2000, 0.2238, 0.2254]),\n",
    "])\n",
    "\n",
    "trainloader.dataset.transform = transform\n",
    "testloader.dataset.transform = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\tinke/.cache\\torch\\hub\\pytorch_vision_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (1): Dropout(p=0.2, inplace=False)\n",
       "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnetmodel = torch.hub.load(\n",
    "    \"pytorch/vision\", \"resnet101\", weights=\"IMAGENET1K_V1\") #resnet18 80%, resnet101 86%\n",
    "num_ftrs = resnetmodel.fc.in_features\n",
    "\n",
    "# Here the size of each output sample is set to 4\n",
    "#resnetmodel.fc = nn.Linear(num_ftrs, 4)\n",
    "\n",
    "# adding dropout cuz its overfitting\n",
    "resnetmodel.fc = torch.nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 512),\n",
    "    torch.nn.Dropout(p=dropout),\n",
    "    torch.nn.Linear(512, 4),\n",
    ")\n",
    "# Check if output size is correct\n",
    "resnetmodel.eval()\n",
    "\n",
    "# self.dense(1000,4)\n",
    "# forward\n",
    "#   dense(model(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load to CUDA\n",
    "myresnetmodel = resnetmodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer = sgd\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "CrossEntropyLossCriterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(myresnetmodel.parameters(), lr=mylearnrate, momentum=mymomentum)\n",
    "if optchoice == 'adam':\n",
    "    optimizer = optim.Adam(myresnetmodel.parameters(), lr=mylearnrate)\n",
    "    print('optimizer = Adam')\n",
    "elif optchoice  == 'sgd':\n",
    "    optimizer = optim.SGD(myresnetmodel.parameters(), lr=mylearnrate, momentum=mymomentum, weight_decay=myweight_decay)\n",
    "    print('optimizer = sgd')\n",
    "elif optchoice  == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(myresnetmodel.parameters(), lr=mylearnrate, momentum=mymomentum)\n",
    "    print('optimizer = rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     26\u001b[0m \u001b[39m#-------------------------\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[39m#forward pass\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m outputs \u001b[39m=\u001b[39m myresnetmodel\u001b[39m.\u001b[39;49mforward(inputs)\n\u001b[0;32m     30\u001b[0m loss \u001b[39m=\u001b[39m CrossEntropyLossCriterion(outputs, labels)\n\u001b[0;32m     32\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# reset previous calculated loss gradients to zero\u001b[39;00m\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torchvision\\models\\resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    267\u001b[0m     \u001b[39m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[0;32m    269\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[0;32m    270\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32md:\\0_Program_Files\\Python3.11\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train function\n",
    "\n",
    "import realtime_graph as graph\n",
    "import gc\n",
    "\n",
    "epochresults = []\n",
    "epoch_valloss = 0 \n",
    "correct = 0\n",
    "total = 0\n",
    "graph.startplot()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_valloss = 0.0\n",
    "\n",
    "    myresnetmodel.train()\n",
    "    for idx, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        # print(inputs.shape) = torch.Size([4, 3, 32, 32])\n",
    "\n",
    "        #---load data into GPU----\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #-------------------------\n",
    "        \n",
    "        #forward pass\n",
    "        outputs = myresnetmodel.forward(inputs)\n",
    "        loss = CrossEntropyLossCriterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()  # reset previous calculated loss gradients to zero\n",
    "        loss.backward() # calculate new loss gradient\n",
    "\n",
    "        optimizer.step() # update weights based on learning rate and gradients\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "\n",
    "    myresnetmodel.eval() # prep model for evaluation\n",
    "    epoch_vallos_previous = epoch_valloss\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for idx, data in enumerate(testloader):\n",
    "\n",
    "            inputs, labels = data\n",
    "            #---load data into GPU----\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #forward pass\n",
    "            outputs = myresnetmodel.forward(inputs)\n",
    "            loss = CrossEntropyLossCriterion(outputs, labels)\n",
    "\n",
    "            # record validation loss\n",
    "            running_valloss += loss.item()\n",
    "\n",
    "            # count correct predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "\n",
    "        epoch_valloss = running_valloss / len(testloader)\n",
    "\n",
    "\n",
    "    epochresults.append([epoch_loss, epoch_valloss])\n",
    "\n",
    "    \n",
    "    graph.realtimeplot(epochresults)\n",
    "\n",
    "   \n",
    "    if epoch_vallos_previous-limit < epoch_valloss < epoch_vallos_previous+limit and epoch_valloss<0.01:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy of the network on the validation set: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_test = \"D:/apple_disease_classification/Test/\"\n",
    "\n",
    "transform_img_normal = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize([resizevalue ,resizevalue]),\n",
    "    transforms.Normalize(mean = [0.6453, 0.4631, 0.3085],\n",
    "                          std= [0.2000, 0.2238, 0.2254]),\n",
    "])\n",
    "dataset_test = ImageFolder(dataset_path_test, transform=transform_img_normal)\n",
    "dataset_test_loader = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=test_batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of the data in dataset_test_loader\n",
    "num_images = len(dataset_test_loader.dataset)\n",
    "print(f\"Number of images: {num_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "confusion_matrix = np.zeros((4, 4))  # Initialize the confusion matrix\n",
    "\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad(): #not calling .backward() == efficiency\n",
    "    #for idx, data in enumerate(dataset_test_loader):\n",
    "    for data in dataset_test_loader:\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # print(inputs.shape) = torch.Size([4, 3, 32, 32])\n",
    "\n",
    "        # ---load data into GPU----\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # -------------------------\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = myresnetmodel.forward(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Print predicted and label values\n",
    "        #for i in range(len(labels)):\n",
    "        #   print(f\"Predicted: {predicted[i]}, Label: {labels[i]}\")\n",
    "            \n",
    "        # Update confusion matrix\n",
    "        for i in range(len(labels)):\n",
    "            confusion_matrix[predicted[i]][labels[i]] += 1\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct // total} %')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = confusion_matrix.shape\n",
    "normtotal = 0\n",
    "normcorrect = confusion_matrix[1][1]\n",
    "for i in range(height):\n",
    "    normtotal += confusion_matrix[i][1]\n",
    "\n",
    "accuracy_normal_apple = round((normcorrect/normtotal)*100, 1)\n",
    "\n",
    "print('Accuracy on Normal Apples:'+str(accuracy_normal_apple)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class labels\n",
    "class_labels = ['Blotch', 'Normal', 'Rot', 'Scab']\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the confusion matrix as an image\n",
    "im = ax.imshow(confusion_matrix, cmap='Greens')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "# Set the axis ticks and labels\n",
    "ax.set_xticks(np.arange(len(class_labels)))\n",
    "ax.set_yticks(np.arange(len(class_labels)))\n",
    "ax.set_xticklabels(class_labels, fontsize=10)\n",
    "ax.set_yticklabels(class_labels, fontsize=10)\n",
    "\n",
    "# Rotate the x-axis labels\n",
    "plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Rotate the y-axis labels\n",
    "plt.setp(ax.get_yticklabels(), rotation=90, ha=\"center\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "\n",
    "# Loop over data dimensions and create text annotations\n",
    "for i in range(len(class_labels)):\n",
    "    for j in range(len(class_labels)):\n",
    "        text = ax.text(j, i, confusion_matrix[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# Set the title\n",
    "ax.set_title(\"Confusion Matrix: 120 Samples\")\n",
    "\n",
    "# Display axis meaning\n",
    "ax.text(0.5, -0.15, f\"True Class\", transform=ax.transAxes,\n",
    "        fontsize=12, ha='center')\n",
    "ax.text(-0.15, 0.3, f\"Predicted Class\", rotation=90, transform=ax.transAxes,\n",
    "        fontsize=12, ha='center')\n",
    "\n",
    "\n",
    "# Show the figure\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = [[21., 4., 0., 1.],\n",
    "                    [0., 18., 0., 1.],\n",
    "                    [0., 2., 37., 4.],\n",
    "                    [9., 0., 1., 22.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision = TP / TP + FP = 18 / (18+1) = 18/19 actual % normal apples of all the predicted normal apples (19) \n",
    "precision = confusion_matrix[1][1] / sum(confusion_matrix[1])\n",
    "print(f'normal apple correctly predicted/precision:',precision)\n",
    "#recall = TP / TP + FN = 18 / (18+4+2) = 18/26 correctly identifying actual normal apples (26)\n",
    "column_sum = sum(row[1] for row in confusion_matrix)\n",
    "recall = confusion_matrix[1][1]/column_sum\n",
    "print(f'identifying normal apples/recall',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(myresnetmodel, 'apple_resnet_classifier.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AQL Classifier\n",
    "def AQLcalc(score):\n",
    "    if score <= 0.4: return 'Class I'\n",
    "    if 0.4 > score <=6.5: return 'Class II'\n",
    "    if 6.5 > score < 15: return 'Class III'\n",
    "    return 'Class IV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "# >>> target = torch.tensor([2, 1, 0, 0])\n",
    "# >>> preds = torch.tensor([\n",
    "# ...   [0.16, 0.26, 0.58],\n",
    "# ...   [0.22, 0.61, 0.17],\n",
    "# ...   [0.71, 0.09, 0.20],\n",
    "# ...   [0.05, 0.82, 0.13],\n",
    "# ... ])\n",
    "# >>> metric = MulticlassConfusionMatrix(num_classes=3)\n",
    "# >>> metric(preds, target)\n",
    "# tensor([[1, 1, 0],\n",
    "#         [0, 1, 0],\n",
    "#         [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.datasets import ImageFolder\n",
    "# from torchvision import transforms\n",
    "# folder_url = r\"D:\\apple_50sample\"\n",
    "\n",
    "# transform_img_normal = transforms.Compose([\n",
    "#     transforms.Resize((224,224)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# dataset = ImageFolder(folder_url, transform=transform_img_normal)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
